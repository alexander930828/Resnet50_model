{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow.keras.layers import Conv2D, Dense, BatchNormalization, Activation\n","from tensorflow.keras.layers import add, Add\n","\n","# identity block은 shortcut 단에 conv layer가 없는 block 영역\n","def identity_block(input_tensor, middle_kernel_size, filters, stage, block):\n","\n","    filter1, filter2, filter3 = filters\n","    \n","    conv_name_base = 'res' + str(stage) + block + '_branch'\n","    bn_name_base = 'bn' + str(stage) + block + '_branch'\n","    \n","    # input -> 1*1 conv -> Batch Norm -> Relu\n","    x = Conv2D(filters=filter1, kernel_size=(1, 1), kernel_initializer='he_normal', name=conv_name_base+'2a')(input_tensor)\n","    x = BatchNormalization(axis=3, name=bn_name_base+'2a')(x)\n","    x = Activation('relu')(x)\n","      \n","    x = Conv2D(filters=filter2, kernel_size=middle_kernel_size, padding='same', kernel_initializer='he_normal', name=conv_name_base+'2b')(x)\n","    x = BatchNormalization(axis=3, name=bn_name_base+'2b')(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(filters=filter3, kernel_size=(1, 1), kernel_initializer='he_normal', name=conv_name_base+'2c')(x)\n","    x = BatchNormalization(axis=3, name=bn_name_base+'2c')(x)\n","    \n","    # Residual Block 수행 결과와 input_tensor를 합한다. \n","    x = Add()([input_tensor, x])\n","    # 또는 x = add([x, input_tensor]) 와 같이 구현할 수도 있음. \n","\n","    # 마지막으로 identity block 내에서 최종 ReLU를 적용\n","    x = Activation('relu')(x)\n","    \n","    return x"]},{"cell_type":"markdown","metadata":{},"source":["### identity block을 연속으로 이어서 하나의 Stage 구성.\n","* 아래는 input tensor의 크기가 feature map 생성시 절반으로 줄지 않음. input tensor의 크기가 절반으로 줄수 있도록 구성 필요.\n","* 동일한 Stage 내에서 feature map의 크기는 그대로 대신, block내에서 filter 개수는 변화"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["input_tensor = Input(shape=(56, 56, 256), name='test_input')\n","x = identity_block(input_tensor, middle_kernel_size=3, filters=[64, 64, 256], stage=2, block='a')\n","x = identity_block(x, middle_kernel_size=3, filters=[64, 64, 256], stage=2, block='b')\n","output = identity_block(x, middle_kernel_size=3, filters=[64, 64, 256], stage=2, block='c')\n","identity_layers = Model(inputs=input_tensor, outputs=output)\n","identity_layers.summary()"]},{"cell_type":"markdown","metadata":{},"source":["### 각 stage내의 첫번째 identity block에서 입력 feature map의 크기를 절반으로 줄이는 block을 생성하는 함수 conv_block() 만들기\n","* conv_block() 함수는 앞에서 구현한 identity_block()함수과 거의 유사하나 입력 feature map의 크기를 절반으로 줄이고 shortcut 전달시 1x1 conv stride 2 적용\n","* 단 첫번째 Stage의 첫번째 block에서는 이미 입력 feature map이 max pool로 절반이 줄어있는 상태이므로 다시 줄이지 않음. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def conv_block(input_tensor, middle_kernel_size, filters, stage, block, strides=(2, 2)):\n","    '''\n","    함수 입력 인자 설명\n","    input_tensor: 입력 tensor\n","    middle_kernel_size: 중간에 위치하는 kernel 크기. identity block내에 있는 두개의 conv layer중 1x1 kernel이 아니고, 3x3 kernel임. \n","                        3x3 커널 이외에도 5x5 kernel도 지정할 수 있게 구성. \n","    filters: 3개 conv layer들의 filter개수를 list 형태로 입력 받음. 첫번째 원소는 첫번째 1x1 filter 개수, 두번째는 3x3 filter 개수, \n","             세번째는 마지막 1x1 filter 개수\n","    stage: identity block들이 여러개가 결합되므로 이를 구분하기 위해서 설정. 동일한 filter수를 가지는 identity block들을  동일한 stage로 설정.  \n","    block: 동일 stage내에서 identity block을 구별하기 위한 구분자\n","    strides: 입력 feature map의 크기를 절반으로 줄이기 위해서 사용. Default는 2이지만, \n","             첫번째 Stage의 첫번째 block에서는 이미 입력 feature map이 max pool로 절반이 줄어있는 상태이므로 다시 줄이지 않기 위해 1을 호출해야함 \n","    ''' \n","    \n","    # filters로 list 형태로 입력된 filter 개수를 각각 filter1, filter2, filter3로 할당. \n","    # filter은 첫번째 1x1 filter 개수, filter2는 3x3 filter개수, filter3는 마지막 1x1 filter개수\n","    filter1, filter2, filter3 = filters\n","    # conv layer와 Batch normalization layer각각에 고유한 이름을 부여하기 위해 설정. 입력받은 stage와 block에 기반하여 이름 부여\n","    conv_name_base = 'res' + str(stage) + block + '_branch'\n","    bn_name_base = 'bn' + str(stage) + block + '_branch'\n","    \n","    # 이전 layer에 입력 받은 input_tensor(함수인자로 입력받음)를 기반으로 첫번째 1x1 Conv->Batch Norm->Relu 수행. \n","    # 입력 feature map 사이즈를 1/2로 줄이기 위해 strides인자를 입력  \n","    x = Conv2D(filters=filter1, kernel_size=(1, 1), strides=strides, kernel_initializer='he_normal', name=conv_name_base+'2a')(input_tensor)\n","    # Batch Norm적용. 입력 데이터는 batch 사이즈까지 포함하여 4차원임(batch_size, height, width, channel depth)임\n","    # Batch Norm의 axis는 channel depth에 해당하는 axis index인 3을 입력.(무조건 channel이 마지막 차원의 값으로 입력된다고 가정. )\n","    x = BatchNormalization(axis=3, name=bn_name_base+'2a')(x)\n","    # ReLU Activation 적용. \n","    x = Activation('relu')(x)\n","    \n","    # 두번째 3x3 Conv->Batch Norm->ReLU 수행\n","    # 3x3이 아닌 다른 kernel size도 구성 가능할 수 있도록 identity_block() 인자로 입력받은 middle_kernel_size를 이용. \n","    # Conv 수행 출력 사이즈가 변하지 않도록 padding='same'으로 설정. filter 개수는 이전의 1x1 filter개수와 동일.  \n","    x = Conv2D(filters=filter2, kernel_size=middle_kernel_size, padding='same', kernel_initializer='he_normal', name=conv_name_base+'2b')(x)\n","    x = BatchNormalization(axis=3, name=bn_name_base+'2b')(x)\n","    x = Activation('relu')(x)\n","    \n","    # 마지막 1x1 Conv->Batch Norm 수행. ReLU를 수행하지 않음에 유의.\n","    # filter 크기는 input_tensor channel 차원 개수로 원복\n","    x = Conv2D(filters=filter3, kernel_size=(1, 1), kernel_initializer='he_normal', name=conv_name_base+'2c')(x)\n","    x = BatchNormalization(axis=3, name=bn_name_base+'2c')(x)\n","    \n","    # shortcut을 1x1 conv 수행, filter3가 입력 feature map의 filter 개수\n","    shortcut = Conv2D(filter3, (1, 1), strides=strides, kernel_initializer='he_normal', name=conv_name_base+'1')(input_tensor)\n","    shortcut = BatchNormalization(axis=3, name=bn_name_base+'1')(shortcut)\n","    \n","    # Residual Block 수행 결과와 1x1 conv가 적용된 shortcut을 합한다. \n","    x = add([x, shortcut])\n","    \n","    # 마지막으로 identity block 내에서 최종 ReLU를 적용\n","    x = Activation('relu')(x)\n","    \n","    return x\n","    "]},{"cell_type":"markdown","metadata":{},"source":["### conv_block()과 identity_block()을 호출하여 stage 구성."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["input_tensor = Input(shape=(56, 56, 256), name='test_input')\n","# conv_block() 호출 시 strides를 2로 설정하여 입력 feature map의 크기를 절반으로 줄임. strides=1이면 크기를 그대로 유지\n","x = conv_block(input_tensor, middle_kernel_size=3, filters=[64, 64, 256], strides=2, stage=2, block='a')\n","x = identity_block(x, middle_kernel_size=3, filters=[64, 64, 256], stage=2, block='b')\n","output = identity_block(x, middle_kernel_size=3, filters=[64, 64, 256], stage=2, block='c')\n","identity_layers = Model(inputs=input_tensor, outputs=output)\n","identity_layers.summary()"]},{"cell_type":"markdown","metadata":{},"source":["### input image를 7x7 Conv 변환하고 Max Pooling 적용 로직을 별도 함수로 구현.\n","* O = (I - F + 2P)/S + 1, I는 Input size, F는 filter의 kernel 크기, P는 padding, S는 Stride\n","* (224 - 7)/2 + 1 = 109.5 = 109가 됨. 따라서 112x112 로 출력하기 위해 ZeroPadding2D(3, 3)수행\n","* 112x112로 MaxPooling 을 (3, 3) pool size로 stride 2로 수행하므로 56x56으로 출력하기 위해 ZeroPadding2D(1,1) 수행"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow.keras.layers import ZeroPadding2D, MaxPooling2D\n","\n","def do_first_conv(input_tensor):\n","    # 7x7 Conv 연산 수행하여 feature map 생성하되 input_tensor 크기(image 크기)의 절반으로 생성.  filter 개수는 64개 \n","    # 224x224 를 input을 7x7 conv, strides=2로 112x112 출력하기 위해 Zero padding 적용. \n","    x = ZeroPadding2D(padding=(3, 3), name='conv1_pad')(input_tensor)\n","    x = Conv2D(64, (7, 7), strides=(2, 2), padding='valid', kernel_initializer='he_normal', name='conv')(x)\n","    x = BatchNormalization(axis=3, name='bn_conv1')(x)\n","    x = Activation('relu')(x)\n","    # 다시 feature map 크기를 MaxPooling으로 절반으로 만듬. 56x56으로 출력하기 위해 zero padding 적용. \n","    x = ZeroPadding2D(padding=(1, 1), name='pool1_pad')(x)\n","    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n","    \n","    return x\n","\n","input_tensor = Input(shape=(224, 224, 3))\n","output = do_first_conv(input_tensor)\n","model = Model(inputs=input_tensor, outputs=output)\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["### ResNet 50 모델 생성.\n","* 앞에서 생성한 conv_block()과 identity_block()을 호출하여 ResNet 50 모델 생성. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D\n","from tensorflow.keras.optimizers import Adam , RMSprop \n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\n","\n","def create_resnet(in_shape=(224, 224, 3), n_classes=10):\n","    input_tensor = Input(shape=in_shape)\n","    \n","    #첫번째 7x7 Conv와 Max Polling 적용.  \n","    x = do_first_conv(input_tensor)\n","    \n","    # stage 2의 conv_block과 identity block 생성. stage2의 첫번째 conv_block은 strides를 1로 하여 크기를 줄이지 않음. \n","    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n","    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n","    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n","    \n","    # stage 3의 conv_block과 identity block 생성. stage3의 첫번째 conv_block은 strides를 2(default)로 하여 크기를 줄임 \n","    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n","    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n","    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n","    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n","\n","    # stage 4의 conv_block과 identity block 생성. stage4의 첫번째 conv_block은 strides를 2(default)로 하여 크기를 줄임\n","    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n","    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n","    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n","    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n","    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n","    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n","\n","    # stage 5의 conv_block과 identity block 생성. stage5의 첫번째 conv_block은 strides를 2(default)로 하여 크기를 줄임\n","    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n","    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n","    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n","    \n","    # classification dense layer와 연결 전 GlobalAveragePooling 수행 \n","    x = GlobalAveragePooling2D(name='avg_pool')(x)\n","    x = Dropout(rate=0.5)(x)\n","    x = Dense(200, activation='relu', name='fc_01')(x)\n","    x = Dropout(rate=0.5)(x)\n","    output = Dense(n_classes, activation='softmax', name='fc_final')(x)\n","    \n","    model = Model(inputs=input_tensor, outputs=output, name='resnet50')\n","    model.summary()\n","    \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model =  create_resnet(in_shape=(224,224,3), n_classes=10)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.7.13","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"vscode":{"interpreter":{"hash":"474cabef28ef5f39850ae0283db27a2b2b64f747313c5bcb210a7b2692fe8216"}}},"nbformat":4,"nbformat_minor":4}
